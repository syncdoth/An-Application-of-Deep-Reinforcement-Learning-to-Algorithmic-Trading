{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import os\n",
    "from collections import OrderedDict, deque, namedtuple\n",
    "from typing import List, Tuple\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "from pytorch_lightning import LightningModule, Trainer\n",
    "from pytorch_lightning.utilities import DistributedType\n",
    "from torch import Tensor, nn\n",
    "from torch.optim import Adam, Optimizer\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import IterableDataset\n",
    "\n",
    "import cv2\n",
    "\n",
    "PATH_DATASETS = os.environ.get(\"./charts\", \".\")\n",
    "AVAIL_GPUS = min(1, torch.cuda.device_count())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# Boolean handling the saving of the stock market data downloaded\n",
    "saving = True\n",
    "\n",
    "# Variable related to the fictive stocks supported\n",
    "fictiveStocks = ('LINEARUP', 'LINEARDOWN', 'SINUSOIDAL', 'TRIANGLE')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class TradingEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    GOAL: Implement a custom trading environment compatible with OpenAI Gym.\n",
    "    \n",
    "    VARIABLES:  - data: Dataframe monitoring the trading activity.\n",
    "                - state: RL state to be returned to the RL agent.\n",
    "                - reward: RL reward to be returned to the RL agent.\n",
    "                - done: RL episode termination signal.\n",
    "                - t: Current trading time step.\n",
    "                - marketSymbol: Stock market symbol.\n",
    "                - startingDate: Beginning of the trading horizon.\n",
    "                - endingDate: Ending of the trading horizon.\n",
    "                - stateLength: Number of trading time steps included in the state.\n",
    "                - numberOfShares: Number of shares currently owned by the agent.\n",
    "                - transactionCosts: Transaction costs associated with the trading\n",
    "                                    activity (e.g. 0.01 is 1% of loss).\n",
    "                                \n",
    "    METHODS:    - __init__: Object constructor initializing the trading environment.\n",
    "                - reset: Perform a soft reset of the trading environment.\n",
    "                - step: Transition to the next trading time step.\n",
    "                - render: Illustrate graphically the trading environment.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, marketSymbol, startingDate, endingDate, money, stateLength=30,\n",
    "                 transactionCosts=0, startingPoint=0):\n",
    "        \"\"\"\n",
    "        GOAL: Object constructor initializing the trading environment by setting up\n",
    "              the trading activity dataframe as well as other important variables.\n",
    "        \n",
    "        INPUTS: - marketSymbol: Stock market symbol.\n",
    "                - startingDate: Beginning of the trading horizon.\n",
    "                - endingDate: Ending of the trading horizon.\n",
    "                - money: Initial amount of money at the disposal of the agent.\n",
    "                - stateLength: Number of trading time steps included in the RL state.\n",
    "                - transactionCosts: Transaction costs associated with the trading\n",
    "                                    activity (e.g. 0.01 is 1% of loss).\n",
    "                - startingPoint: Optional starting point (iteration) of the trading activity.\n",
    "        \n",
    "        OUTPUTS: /\n",
    "        \"\"\"\n",
    " \n",
    "        # CASE 2: Real stock loading\n",
    "        # Check if the stock market data is already present in the database\n",
    "        filename = \"\".join(['Data/', marketSymbol, '_', startingDate, '_', endingDate])\n",
    "        exists = os.path.isfile(filename + '.png')\n",
    "        \n",
    "        # If affirmative, load the stock market data from the database\n",
    "        if(exists):\n",
    "            self.image = cv2.imread(filename)\n",
    "        \n",
    "        # Set the trading activity dataframe\n",
    "        self.data['Position'] = 0\n",
    "        self.data['Action'] = 0\n",
    "        self.data['Holdings'] = 0.\n",
    "        self.data['Cash'] = float(money)\n",
    "        self.data['Money'] = self.data['Holdings'] + self.data['Cash']\n",
    "        self.data['Returns'] = 0.\n",
    "\n",
    "        # Set the RL variables common to every OpenAI gym environments\n",
    "        self.state = self.image\n",
    "        self.reward = 0.\n",
    "        self.done = 0\n",
    "\n",
    "        # Set additional variables related to the trading activity\n",
    "        self.marketSymbol = marketSymbol\n",
    "        self.startingDate = startingDate\n",
    "        self.endingDate = endingDate\n",
    "        self.stateLength = stateLength\n",
    "        self.t = stateLength\n",
    "        self.numberOfShares = 0\n",
    "        self.transactionCosts = transactionCosts\n",
    "        self.epsilon = 0.1\n",
    "\n",
    "        # If required, set a custom starting point for the trading activity\n",
    "        if startingPoint:\n",
    "            self.setStartingPoint(startingPoint)\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        GOAL: Perform a soft reset of the trading environment. \n",
    "        \n",
    "        INPUTS: /    \n",
    "        \n",
    "        OUTPUTS: - state: RL state returned to the trading strategy.\n",
    "        \"\"\"\n",
    "\n",
    "        # Reset the trading activity dataframe\n",
    "        self.data['Position'] = 0\n",
    "        self.data['Action'] = 0\n",
    "        self.data['Holdings'] = 0.\n",
    "        self.data['Cash'] = self.data['Cash'][0]\n",
    "        self.data['Money'] = self.data['Holdings'] + self.data['Cash']\n",
    "        self.data['Returns'] = 0.\n",
    "\n",
    "        # Reset the RL variables common to every OpenAI gym environments\n",
    "        self.state = self.image\n",
    "        self.reward = 0.\n",
    "        self.done = 0\n",
    "\n",
    "        # Reset additional variables related to the trading activity\n",
    "        self.t = self.stateLength\n",
    "        self.numberOfShares = 0\n",
    "\n",
    "        return self.state\n",
    "\n",
    "    \n",
    "    def computeLowerBound(self, cash, numberOfShares, price):\n",
    "        \"\"\"\n",
    "        GOAL: Compute the lower bound of the complete RL action space, \n",
    "              i.e. the minimum number of share to trade.\n",
    "        \n",
    "        INPUTS: - cash: Value of the cash owned by the agent.\n",
    "                - numberOfShares: Number of shares owned by the agent.\n",
    "                - price: Last price observed.\n",
    "        \n",
    "        OUTPUTS: - lowerBound: Lower bound of the RL action space.\n",
    "        \"\"\"\n",
    "\n",
    "        # Computation of the RL action lower bound\n",
    "        deltaValues = - cash - numberOfShares * price * (1 + self.epsilon) * (1 + self.transactionCosts)\n",
    "        if deltaValues < 0:\n",
    "            lowerBound = deltaValues / (price * (2 * self.transactionCosts + (self.epsilon * (1 + self.transactionCosts))))\n",
    "        else:\n",
    "            lowerBound = deltaValues / (price * self.epsilon * (1 + self.transactionCosts))\n",
    "        return lowerBound\n",
    "    \n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        GOAL: Transition to the next trading time step based on the\n",
    "              trading position decision made (either long or short).\n",
    "        \n",
    "        INPUTS: - action: Trading decision (1 = long, 0 = short).    \n",
    "        \n",
    "        OUTPUTS: - state: RL state to be returned to the RL agent.\n",
    "                 - reward: RL reward to be returned to the RL agent.\n",
    "                 - done: RL episode termination signal (boolean).\n",
    "                 - info: Additional information returned to the RL agent.\n",
    "        \"\"\"\n",
    "\n",
    "        # Stting of some local variables\n",
    "        t = self.t\n",
    "        numberOfShares = self.numberOfShares\n",
    "        customReward = False\n",
    "\n",
    "        # CASE 1: LONG POSITION\n",
    "        if(action == 1):\n",
    "            self.data['Position'][t] = 1\n",
    "            # Case a: Long -> Long\n",
    "            if(self.data['Position'][t - 1] == 1):\n",
    "                self.data['Cash'][t] = self.data['Cash'][t - 1]\n",
    "                self.data['Holdings'][t] = self.numberOfShares * self.data['Close'][t]\n",
    "            # Case b: No position -> Long\n",
    "            elif(self.data['Position'][t - 1] == 0):\n",
    "                self.numberOfShares = math.floor(self.data['Cash'][t - 1]/(self.data['Close'][t] * (1 + self.transactionCosts)))\n",
    "                self.data['Cash'][t] = self.data['Cash'][t - 1] - self.numberOfShares * self.data['Close'][t] * (1 + self.transactionCosts)\n",
    "                self.data['Holdings'][t] = self.numberOfShares * self.data['Close'][t]\n",
    "                self.data['Action'][t] = 1\n",
    "            # Case c: Short -> Long\n",
    "            else:\n",
    "                self.data['Cash'][t] = self.data['Cash'][t - 1] - self.numberOfShares * self.data['Close'][t] * (1 + self.transactionCosts)\n",
    "                self.numberOfShares = math.floor(self.data['Cash'][t]/(self.data['Close'][t] * (1 + self.transactionCosts)))\n",
    "                self.data['Cash'][t] = self.data['Cash'][t] - self.numberOfShares * self.data['Close'][t] * (1 + self.transactionCosts)\n",
    "                self.data['Holdings'][t] = self.numberOfShares * self.data['Close'][t]\n",
    "                self.data['Action'][t] = 1\n",
    "\n",
    "        # CASE 2: SHORT POSITION\n",
    "        elif(action == 0):\n",
    "            self.data['Position'][t] = -1\n",
    "            # Case a: Short -> Short\n",
    "            if(self.data['Position'][t - 1] == -1):\n",
    "                lowerBound = self.computeLowerBound(self.data['Cash'][t - 1], -numberOfShares, self.data['Close'][t-1])\n",
    "                if lowerBound <= 0:\n",
    "                    self.data['Cash'][t] = self.data['Cash'][t - 1]\n",
    "                    self.data['Holdings'][t] =  - self.numberOfShares * self.data['Close'][t]\n",
    "                else:\n",
    "                    numberOfSharesToBuy = min(math.floor(lowerBound), self.numberOfShares)\n",
    "                    self.numberOfShares -= numberOfSharesToBuy\n",
    "                    self.data['Cash'][t] = self.data['Cash'][t - 1] - numberOfSharesToBuy * self.data['Close'][t] * (1 + self.transactionCosts)\n",
    "                    self.data['Holdings'][t] =  - self.numberOfShares * self.data['Close'][t]\n",
    "                    customReward = True\n",
    "            # Case b: No position -> Short\n",
    "            elif(self.data['Position'][t - 1] == 0):\n",
    "                self.numberOfShares = math.floor(self.data['Cash'][t - 1]/(self.data['Close'][t] * (1 + self.transactionCosts)))\n",
    "                self.data['Cash'][t] = self.data['Cash'][t - 1] + self.numberOfShares * self.data['Close'][t] * (1 - self.transactionCosts)\n",
    "                self.data['Holdings'][t] = - self.numberOfShares * self.data['Close'][t]\n",
    "                self.data['Action'][t] = -1\n",
    "            # Case c: Long -> Short\n",
    "            else:\n",
    "                self.data['Cash'][t] = self.data['Cash'][t - 1] + self.numberOfShares * self.data['Close'][t] * (1 - self.transactionCosts)\n",
    "                self.numberOfShares = math.floor(self.data['Cash'][t]/(self.data['Close'][t] * (1 + self.transactionCosts)))\n",
    "                self.data['Cash'][t] = self.data['Cash'][t] + self.numberOfShares * self.data['Close'][t] * (1 - self.transactionCosts)\n",
    "                self.data['Holdings'][t] = - self.numberOfShares * self.data['Close'][t]\n",
    "                self.data['Action'][t] = -1\n",
    "\n",
    "        # CASE 3: PROHIBITED ACTION\n",
    "        else:\n",
    "            raise SystemExit(\"Prohibited action! Action should be either 1 (long) or 0 (short).\")\n",
    "\n",
    "        # Update the total amount of money owned by the agent, as well as the return generated\n",
    "        self.data['Money'][t] = self.data['Holdings'][t] + self.data['Cash'][t]\n",
    "        self.data['Returns'][t] = (self.data['Money'][t] - self.data['Money'][t-1])/self.data['Money'][t-1]\n",
    "\n",
    "        # Set the RL reward returned to the trading agent\n",
    "        if not customReward:\n",
    "            self.reward = self.data['Returns'][t]\n",
    "        else:\n",
    "            self.reward = (self.data['Close'][t-1] - self.data['Close'][t])/self.data['Close'][t-1]\n",
    "\n",
    "        # Transition to the next trading time step\n",
    "        self.t = self.t + 1\n",
    "        self.state = [self.data['Close'][self.t - self.stateLength : self.t].tolist(),\n",
    "                      self.data['Low'][self.t - self.stateLength : self.t].tolist(),\n",
    "                      self.data['High'][self.t - self.stateLength : self.t].tolist(),\n",
    "                      self.data['Volume'][self.t - self.stateLength : self.t].tolist(),\n",
    "                      [self.data['Position'][self.t - 1]]]\n",
    "        if(self.t == self.data.shape[0]):\n",
    "            self.done = 1  \n",
    "\n",
    "        # Same reasoning with the other action (exploration trick)\n",
    "        otherAction = int(not bool(action))\n",
    "        customReward = False\n",
    "        if(otherAction == 1):\n",
    "            otherPosition = 1\n",
    "            if(self.data['Position'][t - 1] == 1):\n",
    "                otherCash = self.data['Cash'][t - 1]\n",
    "                otherHoldings = numberOfShares * self.data['Close'][t]\n",
    "            elif(self.data['Position'][t - 1] == 0):\n",
    "                numberOfShares = math.floor(self.data['Cash'][t - 1]/(self.data['Close'][t] * (1 + self.transactionCosts)))\n",
    "                otherCash = self.data['Cash'][t - 1] - numberOfShares * self.data['Close'][t] * (1 + self.transactionCosts)\n",
    "                otherHoldings = numberOfShares * self.data['Close'][t]\n",
    "            else:\n",
    "                otherCash = self.data['Cash'][t - 1] - numberOfShares * self.data['Close'][t] * (1 + self.transactionCosts)\n",
    "                numberOfShares = math.floor(otherCash/(self.data['Close'][t] * (1 + self.transactionCosts)))\n",
    "                otherCash = otherCash - numberOfShares * self.data['Close'][t] * (1 + self.transactionCosts)\n",
    "                otherHoldings = numberOfShares * self.data['Close'][t]\n",
    "        else:\n",
    "            otherPosition = -1\n",
    "            if(self.data['Position'][t - 1] == -1):\n",
    "                lowerBound = self.computeLowerBound(self.data['Cash'][t - 1], -numberOfShares, self.data['Close'][t-1])\n",
    "                if lowerBound <= 0:\n",
    "                    otherCash = self.data['Cash'][t - 1]\n",
    "                    otherHoldings =  - numberOfShares * self.data['Close'][t]\n",
    "                else:\n",
    "                    numberOfSharesToBuy = min(math.floor(lowerBound), numberOfShares)\n",
    "                    numberOfShares -= numberOfSharesToBuy\n",
    "                    otherCash = self.data['Cash'][t - 1] - numberOfSharesToBuy * self.data['Close'][t] * (1 + self.transactionCosts)\n",
    "                    otherHoldings =  - numberOfShares * self.data['Close'][t]\n",
    "                    customReward = True\n",
    "            elif(self.data['Position'][t - 1] == 0):\n",
    "                numberOfShares = math.floor(self.data['Cash'][t - 1]/(self.data['Close'][t] * (1 + self.transactionCosts)))\n",
    "                otherCash = self.data['Cash'][t - 1] + numberOfShares * self.data['Close'][t] * (1 - self.transactionCosts)\n",
    "                otherHoldings = - numberOfShares * self.data['Close'][t]\n",
    "            else:\n",
    "                otherCash = self.data['Cash'][t - 1] + numberOfShares * self.data['Close'][t] * (1 - self.transactionCosts)\n",
    "                numberOfShares = math.floor(otherCash/(self.data['Close'][t] * (1 + self.transactionCosts)))\n",
    "                otherCash = otherCash + numberOfShares * self.data['Close'][t] * (1 - self.transactionCosts)\n",
    "                otherHoldings = - self.numberOfShares * self.data['Close'][t]\n",
    "        otherMoney = otherHoldings + otherCash\n",
    "        if not customReward:\n",
    "            otherReward = (otherMoney - self.data['Money'][t-1])/self.data['Money'][t-1]\n",
    "        else:\n",
    "            otherReward = (self.data['Close'][t-1] - self.data['Close'][t])/self.data['Close'][t-1]\n",
    "        otherState = [self.data['Close'][self.t - self.stateLength : self.t].tolist(),\n",
    "                      self.data['Low'][self.t - self.stateLength : self.t].tolist(),\n",
    "                      self.data['High'][self.t - self.stateLength : self.t].tolist(),\n",
    "                      self.data['Volume'][self.t - self.stateLength : self.t].tolist(),\n",
    "                      [otherPosition]]\n",
    "        self.info = {'State' : otherState, 'Reward' : otherReward, 'Done' : self.done}\n",
    "\n",
    "        # Return the trading environment feedback to the RL trading agent\n",
    "        return self.state, self.reward, self.done, self.info\n",
    "\n",
    "\n",
    "    def render(self):\n",
    "        \"\"\"\n",
    "        GOAL: Illustrate graphically the trading activity, by plotting\n",
    "              both the evolution of the stock market price and the \n",
    "              evolution of the trading capital. All the trading decisions\n",
    "              (long and short positions) are displayed as well.\n",
    "        \n",
    "        INPUTS: /   \n",
    "        \n",
    "        OUTPUTS: /\n",
    "        \"\"\"\n",
    "\n",
    "        # Set the Matplotlib figure and subplots\n",
    "        fig = plt.figure(figsize=(10, 8))\n",
    "        ax1 = fig.add_subplot(211, ylabel='Price', xlabel='Time')\n",
    "        ax2 = fig.add_subplot(212, ylabel='Capital', xlabel='Time', sharex=ax1)\n",
    "\n",
    "        # Plot the first graph -> Evolution of the stock market price\n",
    "        self.data['Close'].plot(ax=ax1, color='blue', lw=2)\n",
    "        ax1.plot(self.data.loc[self.data['Action'] == 1.0].index, \n",
    "                 self.data['Close'][self.data['Action'] == 1.0],\n",
    "                 '^', markersize=5, color='green')   \n",
    "        ax1.plot(self.data.loc[self.data['Action'] == -1.0].index, \n",
    "                 self.data['Close'][self.data['Action'] == -1.0],\n",
    "                 'v', markersize=5, color='red')\n",
    "        \n",
    "        # Plot the second graph -> Evolution of the trading capital\n",
    "        self.data['Money'].plot(ax=ax2, color='blue', lw=2)\n",
    "        ax2.plot(self.data.loc[self.data['Action'] == 1.0].index, \n",
    "                 self.data['Money'][self.data['Action'] == 1.0],\n",
    "                 '^', markersize=5, color='green')   \n",
    "        ax2.plot(self.data.loc[self.data['Action'] == -1.0].index, \n",
    "                 self.data['Money'][self.data['Action'] == -1.0],\n",
    "                 'v', markersize=5, color='red')\n",
    "        \n",
    "        # Generation of the two legends and plotting\n",
    "        ax1.legend([\"Price\", \"Long\",  \"Short\"])\n",
    "        ax2.legend([\"Capital\", \"Long\", \"Short\"])\n",
    "        plt.savefig(''.join(['Figures/', str(self.marketSymbol), '_Rendering', '.png']))\n",
    "        #plt.show()\n",
    "\n",
    "\n",
    "    def setStartingPoint(self, startingPoint):\n",
    "        \"\"\"\n",
    "        GOAL: Setting an arbitrary starting point regarding the trading activity.\n",
    "              This technique is used for better generalization of the RL agent.\n",
    "        \n",
    "        INPUTS: - startingPoint: Optional starting point (iteration) of the trading activity.\n",
    "        \n",
    "        OUTPUTS: /\n",
    "        \"\"\"\n",
    "\n",
    "        # Setting a custom starting point\n",
    "        self.t = np.clip(startingPoint, self.stateLength, len(self.data.index))\n",
    "\n",
    "        # Set the RL variables common to every OpenAI gym environments\n",
    "        self.state = [self.data['Close'][self.t - self.stateLength : self.t].tolist(),\n",
    "                      self.data['Low'][self.t - self.stateLength : self.t].tolist(),\n",
    "                      self.data['High'][self.t - self.stateLength : self.t].tolist(),\n",
    "                      self.data['Volume'][self.t - self.stateLength : self.t].tolist(),\n",
    "                      [self.data['Position'][self.t - 1]]]\n",
    "        if(self.t == self.data.shape[0]):\n",
    "            self.done = 1\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "class DQN(nn.Module):\n",
    "    \"\"\"Simple MLP network.\"\"\"\n",
    "\n",
    "    def __init__(self, obs_size: int, n_actions: int, hidden_size: int = 128):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            obs_size: observation/state size of the environment\n",
    "            n_actions: number of discrete actions available in the environment\n",
    "            hidden_size: size of hidden layers\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, n_actions),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x.float())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# Named tuple for storing experience steps gathered in training\n",
    "Experience = namedtuple(\n",
    "    \"Experience\",\n",
    "    field_names=[\"state\", \"action\", \"reward\", \"done\", \"new_state\"],\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"Replay Buffer for storing past experiences allowing the agent to learn from them.\n",
    "\n",
    "    Args:\n",
    "        capacity: size of the buffer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, capacity: int) -> None:\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def __len__(self) -> None:\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def append(self, experience: Experience) -> None:\n",
    "        \"\"\"Add experience to the buffer.\n",
    "\n",
    "        Args:\n",
    "            experience: tuple (state, action, reward, done, new_state)\n",
    "        \"\"\"\n",
    "        self.buffer.append(experience)\n",
    "\n",
    "    def sample(self, batch_size: int) -> Tuple:\n",
    "        indices = np.random.choice(len(self.buffer), batch_size, replace=False)\n",
    "        states, actions, rewards, dones, next_states = zip(*(self.buffer[idx] for idx in indices))\n",
    "\n",
    "        return (\n",
    "            np.array(states),\n",
    "            np.array(actions),\n",
    "            np.array(rewards, dtype=np.float32),\n",
    "            np.array(dones, dtype=np.bool),\n",
    "            np.array(next_states),\n",
    "        )"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "class RLDataset(IterableDataset):\n",
    "    \"\"\"Iterable Dataset containing the ExperienceBuffer which will be updated with new experiences during training.\n",
    "\n",
    "    Args:\n",
    "        buffer: replay buffer\n",
    "        sample_size: number of experiences to sample at a time\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, buffer: ReplayBuffer, sample_size: int = 200) -> None:\n",
    "        self.buffer = buffer\n",
    "        self.sample_size = sample_size\n",
    "\n",
    "    def __iter__(self) -> Tuple:#Tuple\n",
    "        states, actions, rewards, dones, new_states = self.buffer.sample(self.sample_size)\n",
    "        for i in range(len(dones)):\n",
    "            yield states[i], actions[i], rewards[i], dones[i], new_states[i]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "class Agent:\n",
    "    \"\"\"Base Agent class handeling the interaction with the environment.\"\"\"\n",
    "\n",
    "    def __init__(self, env: gym.Env, replay_buffer: ReplayBuffer) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            env: training environment\n",
    "            replay_buffer: replay buffer storing experiences\n",
    "        \"\"\"\n",
    "        self.env = env\n",
    "        self.replay_buffer = replay_buffer\n",
    "        self.reset()\n",
    "        self.state = self.env.reset()\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        \"\"\"Resents the environment and updates the state.\"\"\"\n",
    "        self.state = self.env.reset()\n",
    "\n",
    "    def get_action(self, net: nn.Module, epsilon: float, device: str) -> int:\n",
    "        \"\"\"Using the given network, decide what action to carry out using an epsilon-greedy policy.\n",
    "\n",
    "        Args:\n",
    "            net: DQN network\n",
    "            epsilon: value to determine likelihood of taking a random action\n",
    "            device: current device\n",
    "\n",
    "        Returns:\n",
    "            action\n",
    "        \"\"\"\n",
    "        if np.random.random() < epsilon:\n",
    "            action = self.env.action_space.sample()\n",
    "        else:\n",
    "            state = torch.tensor([self.state])\n",
    "\n",
    "            if device not in [\"cpu\"]:\n",
    "                state = state.cuda(device)\n",
    "\n",
    "            q_values = net(state)\n",
    "            _, action = torch.max(q_values, dim=1)\n",
    "            action = int(action.item())\n",
    "\n",
    "        return action\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def play_step(\n",
    "        self,\n",
    "        net: nn.Module,\n",
    "        epsilon: float = 0.0,\n",
    "        device: str = \"cpu\",\n",
    "    ) -> Tuple[float, bool]:\n",
    "        \"\"\"Carries out a single interaction step between the agent and the environment.\n",
    "\n",
    "        Args:\n",
    "            net: DQN network\n",
    "            epsilon: value to determine likelihood of taking a random action\n",
    "            device: current device\n",
    "\n",
    "        Returns:\n",
    "            reward, done\n",
    "        \"\"\"\n",
    "\n",
    "        action = self.get_action(net, epsilon, device)\n",
    "\n",
    "        # do step in the environment\n",
    "        new_state, reward, done, _ = self.env.step(action)\n",
    "\n",
    "        exp = Experience(self.state, action, reward, done, new_state)\n",
    "\n",
    "        self.replay_buffer.append(exp)\n",
    "\n",
    "        self.state = new_state\n",
    "        if done:\n",
    "            self.reset()\n",
    "        return reward, done"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "class DQNLightning(LightningModule):\n",
    "    \"\"\"Basic DQN Model.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        batch_size: int = 16,\n",
    "        lr: float = 1e-2,\n",
    "        env: str = \"CartPole-v0\",\n",
    "        gamma: float = 0.99,\n",
    "        sync_rate: int = 10,\n",
    "        replay_size: int = 1000,\n",
    "        warm_start_size: int = 1000,\n",
    "        eps_last_frame: int = 1000,\n",
    "        eps_start: float = 1.0,\n",
    "        eps_end: float = 0.01,\n",
    "        episode_length: int = 200,\n",
    "        warm_start_steps: int = 1000,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            batch_size: size of the batches\")\n",
    "            lr: learning rate\n",
    "            env: gym environment tag\n",
    "            gamma: discount factor\n",
    "            sync_rate: how many frames do we update the target network\n",
    "            replay_size: capacity of the replay buffer\n",
    "            warm_start_size: how many samples do we use to fill our buffer at the start of training\n",
    "            eps_last_frame: what frame should epsilon stop decaying\n",
    "            eps_start: starting value of epsilon\n",
    "            eps_end: final value of epsilon\n",
    "            episode_length: max length of an episode\n",
    "            warm_start_steps: max episode reward in the environment\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.env = gym.make(self.hparams.env)\n",
    "        obs_size = self.env.observation_space.shape[0]\n",
    "        n_actions = self.env.action_space.n\n",
    "\n",
    "        self.net = DQN(obs_size, n_actions)\n",
    "        self.target_net = DQN(obs_size, n_actions)\n",
    "\n",
    "        self.buffer = ReplayBuffer(self.hparams.replay_size)\n",
    "        self.agent = Agent(self.env, self.buffer)\n",
    "        self.total_reward = 0\n",
    "        self.episode_reward = 0\n",
    "        self.populate(self.hparams.warm_start_steps)\n",
    "\n",
    "    def populate(self, steps: int = 1000) -> None:\n",
    "        \"\"\"Carries out several random steps through the environment to initially fill up the replay buffer with\n",
    "        experiences.\n",
    "\n",
    "        Args:\n",
    "            steps: number of random steps to populate the buffer with\n",
    "        \"\"\"\n",
    "        for i in range(steps):\n",
    "            self.agent.play_step(self.net, epsilon=1.0)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"Passes in a state x through the network and gets the q_values of each action as an output.\n",
    "\n",
    "        Args:\n",
    "            x: environment state\n",
    "\n",
    "        Returns:\n",
    "            q values\n",
    "        \"\"\"\n",
    "        output = self.net(x)\n",
    "        return output\n",
    "\n",
    "    def dqn_mse_loss(self, batch: Tuple[Tensor, Tensor]) -> Tensor:\n",
    "        \"\"\"Calculates the mse loss using a mini batch from the replay buffer.\n",
    "\n",
    "        Args:\n",
    "            batch: current mini batch of replay data\n",
    "\n",
    "        Returns:\n",
    "            loss\n",
    "        \"\"\"\n",
    "        states, actions, rewards, dones, next_states = batch\n",
    "\n",
    "        state_action_values = self.net(states).gather(1, actions.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            next_state_values = self.target_net(next_states).max(1)[0]\n",
    "            next_state_values[dones] = 0.0\n",
    "            next_state_values = next_state_values.detach()\n",
    "\n",
    "        expected_state_action_values = next_state_values * self.hparams.gamma + rewards\n",
    "\n",
    "        return nn.MSELoss()(state_action_values, expected_state_action_values)\n",
    "\n",
    "    def training_step(self, batch: Tuple[Tensor, Tensor], nb_batch) -> OrderedDict:\n",
    "        \"\"\"Carries out a single step through the environment to update the replay buffer. Then calculates loss\n",
    "        based on the minibatch recieved.\n",
    "\n",
    "        Args:\n",
    "            batch: current mini batch of replay data\n",
    "            nb_batch: batch number\n",
    "\n",
    "        Returns:\n",
    "            Training loss and log metrics\n",
    "        \"\"\"\n",
    "        device = self.get_device(batch)\n",
    "        epsilon = max(\n",
    "            self.hparams.eps_end,\n",
    "            self.hparams.eps_start - self.global_step + 1 / self.hparams.eps_last_frame,\n",
    "        )\n",
    "\n",
    "        # step through environment with agent\n",
    "        reward, done = self.agent.play_step(self.net, epsilon, device)\n",
    "        self.episode_reward += reward\n",
    "\n",
    "        # calculates training loss\n",
    "        loss = self.dqn_mse_loss(batch)\n",
    "\n",
    "        if self.trainer._distrib_type in {DistributedType.DP, DistributedType.DDP2}:\n",
    "            loss = loss.unsqueeze(0)\n",
    "\n",
    "        if done:\n",
    "            self.total_reward = self.episode_reward\n",
    "            self.episode_reward = 0\n",
    "\n",
    "        # Soft update of target network\n",
    "        if self.global_step % self.hparams.sync_rate == 0:\n",
    "            self.target_net.load_state_dict(self.net.state_dict())\n",
    "\n",
    "        log = {\n",
    "            \"total_reward\": torch.tensor(self.total_reward).to(device),\n",
    "            \"reward\": torch.tensor(reward).to(device),\n",
    "            \"train_loss\": loss,\n",
    "        }\n",
    "        status = {\n",
    "            \"steps\": torch.tensor(self.global_step).to(device),\n",
    "            \"total_reward\": torch.tensor(self.total_reward).to(device),\n",
    "        }\n",
    "\n",
    "        return OrderedDict({\"loss\": loss, \"log\": log, \"progress_bar\": status})\n",
    "\n",
    "    def configure_optimizers(self) -> List[Optimizer]:\n",
    "        \"\"\"Initialize Adam optimizer.\"\"\"\n",
    "        optimizer = Adam(self.net.parameters(), lr=self.hparams.lr)\n",
    "        return [optimizer]\n",
    "\n",
    "    def __dataloader(self) -> DataLoader:\n",
    "        \"\"\"Initialize the Replay Buffer dataset used for retrieving experiences.\"\"\"\n",
    "        dataset = RLDataset(self.buffer, self.hparams.episode_length)\n",
    "        dataloader = DataLoader(\n",
    "            dataset=dataset,\n",
    "            batch_size=self.hparams.batch_size,\n",
    "        )\n",
    "        return dataloader\n",
    "\n",
    "    def train_dataloader(self) -> DataLoader:\n",
    "        \"\"\"Get train loader.\"\"\"\n",
    "        return self.__dataloader()\n",
    "\n",
    "    def get_device(self, batch) -> str:\n",
    "        \"\"\"Retrieve device currently being used by minibatch.\"\"\"\n",
    "        return batch[0].device.index if self.on_gpu else \"cpu\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model = DQNLightning()\n",
    "\n",
    "trainer = Trainer(\n",
    "    gpus=AVAIL_GPUS,\n",
    "    max_epochs=200,\n",
    "    val_check_interval=100,\n",
    ")\n",
    "\n",
    "trainer.fit(model)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.6.8",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.8 64-bit"
  },
  "interpreter": {
   "hash": "4cd7ab41f5fca4b9b44701077e38c5ffd31fe66a6cab21e0214b68d958d0e462"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}